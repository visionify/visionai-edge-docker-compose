version: '3.8'

services:
  # Prometheus service for monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: visionai-prometheus
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus:/etc/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    ports:
      - '9090:9090'
    networks:
      - visionai-network

  # Grafana service for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: visionai-grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=secret
      - GF_SERVER_HTTP_PORT=9091
    ports:
      - '9091:9091'
    networks:
      - visionai-network

  # Triton Inference Server
  visionai-triton:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    platform: linux/amd64
    container_name: visionai-triton
    restart: unless-stopped
    ports:
      - '8000:8000'
      - '8001:8001'
      - '8002:8002'
    volumes:
      - ${HOME}/.visionai/models-repo:/models
    environment:
      - CUDA_VISIBLE_DEVICES=-1  # Disable GPU
    command: [
      "tritonserver",
      "--model-store=/models",
      "--allow-http=true",
      "--allow-grpc=true",
      "--allow-metrics=true",
    ]
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
    networks:
      - visionai-network

  # VisionAI Inference Instances
  visionai-inference-1:
    image: visionify/visionai-inference:test
    container_name: visionai-inference-1
    restart: unless-stopped
    env_file:
      - ./.env
    ports:
      - '3002:3002'
      - '3003:3003'
    volumes:
      - ./.env:/App/.env
      - ${HOME}/.visionai:/App/.visionai
    environment:
      HOST_PATH: /App/.visionai
      INSTANCE_ID: "1"
    networks:
      - visionai-network
    depends_on:
      - visionai-triton

  visionai-inference-2:
    image: visionify/visionai-inference:test
    container_name: visionai-inference-2
    restart: unless-stopped
    env_file:
      - ./.env
    ports:
      - '3012:3012'
      - '3013:3013'
    volumes:
      - ./.env:/App/.env
      - ${HOME}/.visionai:/App/.visionai
    environment:
      HOST_PATH: /App/.visionai
      INSTANCE_ID: "2"
    networks:
      - visionai-network
    depends_on:
      - visionai-triton

  # VisionAI Screenshots Service
  visionai-screenshots:
    image: visionify/visionai-screenshots:latest
    container_name: visionai-screenshots
    restart: unless-stopped
    env_file:
      - ./.env
    networks:
      - visionai-network

networks:
  visionai-network:
    driver: bridge

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
